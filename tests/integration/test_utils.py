"""
æµ‹è¯•å·¥å…·å‡½æ•°
"""

import asyncio
import time
import re
from typing import Any, Callable, Optional, Dict
from datetime import datetime
import json

from test_config import TestConfig


async def run_with_timeout_and_retry(
    func: Callable,
    timeout: float,
    max_retries: int = None,
    retry_delay: float = None
) -> Any:
    """
    å¸¦è¶…æ—¶å’Œé‡è¯•çš„å¼‚æ­¥å‡½æ•°æ‰§è¡Œ

    Args:
        func: è¦æ‰§è¡Œçš„å¼‚æ­¥å‡½æ•°
        timeout: è¶…æ—¶æ—¶é—´(ç§’)
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆNoneåˆ™ä½¿ç”¨configé»˜è®¤å€¼ï¼‰
        retry_delay: é‡è¯•å»¶è¿Ÿ(ç§’)ï¼ˆNoneåˆ™ä½¿ç”¨configé»˜è®¤å€¼ï¼‰

    Returns:
        å‡½æ•°æ‰§è¡Œç»“æœ

    Raises:
        asyncio.TimeoutError: æ‰€æœ‰é‡è¯•éƒ½è¶…æ—¶
        Exception: å‡½æ•°æ‰§è¡Œå¼‚å¸¸
    """
    if max_retries is None:
        max_retries = TestConfig.MAX_RETRIES
    if retry_delay is None:
        retry_delay = TestConfig.RETRY_DELAY

    for attempt in range(max_retries):
        try:
            result = await asyncio.wait_for(func(), timeout=timeout)
            return result

        except asyncio.TimeoutError:
            if attempt < max_retries - 1:
                print(f"  â±ï¸  Timeout (attempt {attempt + 1}/{max_retries}), retrying...")
                await asyncio.sleep(retry_delay)
            else:
                print(f"  âŒ Timeout after {max_retries} attempts ({timeout}s each)")
                raise

        except Exception as e:
            if attempt < max_retries - 1:
                print(f"  âš ï¸  Error: {e}, retrying...")
                await asyncio.sleep(retry_delay)
            else:
                raise


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é•¿"""
    if seconds < 1:
        return f"{seconds * 1000:.0f}ms"
    elif seconds < 60:
        return f"{seconds:.1f}s"
    else:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}m{secs:.0f}s"


def print_test_header(section_num: int, section_name: str):
    """æ‰“å°æµ‹è¯•èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f"Section {section_num}: {section_name}")
    print("=" * 70 + "\n")


def print_test_case(case_num: int, user_input: str):
    """æ‰“å°æµ‹è¯•ç”¨ä¾‹"""
    print(f"\nTest Case {case_num}: \"{user_input}\"")


def print_result(success: bool, message: str, indent: int = 2):
    """æ‰“å°æµ‹è¯•ç»“æœ"""
    symbol = "âœ…" if success else "âŒ"
    spaces = " " * indent
    print(f"{spaces}{symbol} {message}")


def print_section_summary(results: Dict[str, Any]):
    """æ‰“å°èŠ‚æµ‹è¯•æ‘˜è¦"""
    total = results.get("total", 0)
    passed = results.get("passed", 0)
    failed = results.get("failed", 0)
    duration = results.get("duration", 0)
    tool_calls = results.get("tool_calls", 0)

    print("\n" + "-" * 70)
    print("Section Summary:")
    print(f"  Total: {total} tests")
    print(f"  âœ… Passed: {passed}")
    print(f"  âŒ Failed: {failed}")
    print(f"  â±ï¸  Total Time: {format_duration(duration)}")
    if tool_calls > 0:
        print(f"  ğŸ”§ Tool Calls: {tool_calls}")
    print("-" * 70)


def verify_response_pattern(response: str, pattern: str) -> bool:
    """
    éªŒè¯å“åº”æ˜¯å¦åŒ¹é…é¢„æœŸæ¨¡å¼

    Args:
        response: å“åº”æ–‡æœ¬
        pattern: æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼

    Returns:
        æ˜¯å¦åŒ¹é…
    """
    return bool(re.search(pattern, response, re.IGNORECASE | re.DOTALL))


def verify_ambiguous_response(
    response: str,
    tool_calls: list,
    acceptable_outcomes: list
) -> str:
    """
    éªŒè¯æ¨¡ç³Šå‘½ä»¤çš„å“åº”

    Args:
        response: å“åº”æ–‡æœ¬
        tool_calls: å·¥å…·è°ƒç”¨åˆ—è¡¨
        acceptable_outcomes: å¯æ¥å—çš„ç»“æœç±»å‹åˆ—è¡¨

    Returns:
        è¯†åˆ«å‡ºçš„ç»“æœç±»å‹
    """
    response_lower = response.lower()

    # æ£€æŸ¥æ˜¯å¦è¯·æ±‚æ¾„æ¸…
    clarification_keywords = [
        "how much", "how many", "which", "where", "what",
        "clarify", "specify", "more details", "could you",
        "can you tell me", "need to know"
    ]
    if any(keyword in response_lower for keyword in clarification_keywords):
        return "clarification_request"

    # æ£€æŸ¥æ˜¯å¦ç¤¼è²Œæ‹’ç»
    decline_keywords = [
        "cannot", "can't", "unable", "unclear",
        "don't understand", "not sure", "need more",
        "ambiguous", "vague"
    ]
    if any(keyword in response_lower for keyword in decline_keywords):
        return "polite_decline"

    # æ£€æŸ¥æ˜¯å¦åˆ—å‡ºé€‰é¡¹
    if any(word in response_lower for word in ["available", "options", "locations"]):
        return "list_available_locations"

    # æ£€æŸ¥æ˜¯å¦å°è¯•åˆç†è§£é‡Š
    if len(tool_calls) > 0:
        return "reasonable_interpretation"

    return "unknown"


def calculate_coordinate_error(
    actual: Dict[str, float],
    expected: Dict[str, float]
) -> Dict[str, float]:
    """
    è®¡ç®—åæ ‡è¯¯å·®

    Args:
        actual: å®é™…åæ ‡ {"x": ..., "y": ..., "yaw": ...}
        expected: é¢„æœŸåæ ‡

    Returns:
        è¯¯å·®å­—å…¸ {"x": ..., "y": ..., "yaw": ...}
    """
    errors = {}

    for key in ["x", "y", "yaw"]:
        if key in expected:
            actual_val = actual.get(key, 0)
            expected_val = expected[key]
            errors[key] = abs(actual_val - expected_val)

    return errors


def generate_test_report(
    test_run_id: str,
    sections: list,
    total_duration: float
) -> Dict[str, Any]:
    """
    ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š

    Args:
        test_run_id: æµ‹è¯•è¿è¡ŒID
        sections: å„èŠ‚æµ‹è¯•ç»“æœ
        total_duration: æ€»æ—¶é•¿

    Returns:
        æŠ¥å‘Šå­—å…¸
    """
    total_tests = sum(s.get("total", 0) for s in sections)
    passed_tests = sum(s.get("passed", 0) for s in sections)
    failed_tests = sum(s.get("failed", 0) for s in sections)
    passed_sections = sum(1 for s in sections if s.get("failed", 0) == 0)

    return {
        "test_run_id": test_run_id,
        "timestamp": datetime.now().isoformat(),
        "config": TestConfig.get_summary(),
        "sections": sections,
        "overall_summary": {
            "total_sections": len(sections),
            "passed_sections": passed_sections,
            "failed_sections": len(sections) - passed_sections,
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "pass_rate": f"{passed_tests/total_tests*100:.1f}%" if total_tests > 0 else "0%",
            "total_duration": total_duration,
            "total_duration_formatted": format_duration(total_duration)
        }
    }


def save_json_report(report: Dict[str, Any], filename: str):
    """ä¿å­˜JSONæ ¼å¼æŠ¥å‘Š"""
    filepath = TestConfig.REPORT_DIR / f"{filename}.json"
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ“„ JSON report saved: {filepath}")


def save_markdown_report(report: Dict[str, Any], filename: str):
    """ä¿å­˜Markdownæ ¼å¼æŠ¥å‘Š"""
    filepath = TestConfig.REPORT_DIR / f"{filename}.md"

    with open(filepath, "w", encoding="utf-8") as f:
        # æ ‡é¢˜
        f.write("# Agent System Test Report\n\n")
        f.write(f"**Test Run ID**: {report['test_run_id']}\n\n")
        f.write(f"**Timestamp**: {report['timestamp']}\n\n")

        # æ€»ä½“æ‘˜è¦
        summary = report["overall_summary"]
        f.write("## Overall Summary\n\n")
        f.write(f"- **Total Sections**: {summary['total_sections']}\n")
        f.write(f"- **Passed Sections**: {summary['passed_sections']}\n")
        f.write(f"- **Total Tests**: {summary['total_tests']}\n")
        f.write(f"- **Passed Tests**: {summary['passed_tests']} ({summary['pass_rate']})\n")
        f.write(f"- **Failed Tests**: {summary['failed_tests']}\n")
        f.write(f"- **Total Duration**: {summary['total_duration_formatted']}\n\n")

        # å„èŠ‚è¯¦æƒ…
        f.write("## Section Details\n\n")
        for section in report["sections"]:
            status = "âœ… PASSED" if section.get("failed", 0) == 0 else "âŒ FAILED"
            f.write(f"### Section {section['id']}: {section['name']} {status}\n\n")
            f.write(f"- Tests: {section['total']}\n")
            f.write(f"- Passed: {section['passed']}\n")
            f.write(f"- Failed: {section['failed']}\n")
            f.write(f"- Duration: {format_duration(section['duration'])}\n")

            if "tool_calls" in section:
                f.write(f"- Tool Calls: {section['tool_calls']}\n")

            f.write("\n")

    print(f"ğŸ“„ Markdown report saved: {filepath}")


def print_final_summary(report: Dict[str, Any]):
    """æ‰“å°æœ€ç»ˆæ‘˜è¦"""
    summary = report["overall_summary"]

    print("\n" + "=" * 70)
    print(" Final Test Summary")
    print("=" * 70 + "\n")

    print(f"Total Sections: {summary['total_sections']}")
    print(f"âœ… Passed Sections: {summary['passed_sections']}")
    print(f"âŒ Failed Sections: {summary['failed_sections']}")
    print()
    print(f"Total Tests: {summary['total_tests']}")
    print(f"âœ… Passed: {summary['passed_tests']} ({summary['pass_rate']})")
    print(f"âŒ Failed: {summary['failed_tests']}")
    print()
    print(f"â±ï¸  Total Duration: {summary['total_duration_formatted']}")

    if summary["failed_tests"] == 0:
        print("\n" + "ğŸ‰" * 35)
        print("All tests passed successfully!")
        print("ğŸ‰" * 35)
    else:
        print("\nâš ï¸  Some tests failed. Check the report for details.")

    print("\n" + "=" * 70 + "\n")
